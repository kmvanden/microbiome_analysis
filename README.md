# Microbiome Analysis
## :microbe::chart_with_upwards_trend: Why Microbiome Data Are Statistically Challenging
**Compositionality**: microbiome data represent relative abundances, not absolute counts, which means that the data are constrained. Since all values are non-negative and sum to a constraint, the data exist in a simplex, not in standard Euclidean space. Furthermore, since the total abundance is fixed, spurious correlations and dependencies are created between taxa (i.e., an increase in the proportion of one taxa must be accompanied by decreases in the others, even if their absolute abundances did not change). Many statistical models assume that variables are independent and unconstrained in Euclidean space, and microbiome data violate these assumptions.
  - **Solution**: Log-ratio transformations move compositional data from the simplex into a standard Euclidean (unconstrained) space suitable for standard statistics. Alternatively, some methods model compositionality explicitly, like ALDEx2, which uses CLR transformation and Monte Carlo sampling from a Dirichlet-multinomial distribution.

**High dimensionality**: microbiome datasets typically contain many more features (taxa) than samples (p >> n), whereas most standard statistical tests assume that the number of samples exceeds the number of features. In small datasets, chance fluctuations in relative abundances can appear as significant differences and high numbers of features relative to the number of samples increases the risk of this happening. Additionally, with high dimensional data, the regression problem becomes undetermined: you have more unknowns (coefficients) than equations (observations), so there are an infinite number of solutions for the coefficients. In logistic regression, this creates a flat optimization surface (ill-conditioned), which can result in non-convergence, numerical instability (very large coefficients) and overfitting.
  - **Solution**: Dimensionality reduction techniques (e.g., PCA or PCoA) reduce the number of features to a smaller set of composite (latent) features. Feature selection by filtering low-abundance or low-prevalence taxa, regularization techniques (e.g., Lasso) or recursive feature elimination can help to identify a smaller subset of features to be used in subsequent analyses.

**Overdispersion and zero-inflation**: in microbiome data, most taxa are absent (zero counts) in most samples, due to both rare taxa being below detection thresholds (rounded or sampling zeros) and due to true biological absence of certain taxa (true or structural zeros). Sparsity is a major contributor to the overdispersion observed in microbiome count data: a species may be absent in most samples, but have very large values in other samples when it is present. Overdispersion violates the assumptions made by models used with count data, like the Poisson distribution, which assume that the variance does not exceed the mean. Additionally, the sparsity of microbiome data also results in zero-inflation (i.e., more zeros than expected under typical count distributions), due to the presence of both sampling and structural zeros.
  - **Solution**: negative binomial models (e.g., in DESeq2) have a dispersion parameter that allows them to model variance independently from the mean and to therefore capture the overdispersion present in microbiome data. Zero-inflation can also be controlled for by using zero-inflated negative binomial (ZINB) models, which explicitly model the zero counts as separate processes (a structural zero process and a negative binomial count process). Additionally, feature filtering before analysis can help to reduce the impact of zero-inflation. 

## :moneybag::balance_scale: Alpha Diversity
Alpha diversity measures the diversity within a single sample. Traditional alpha diversity metrics come from classical ecology and include: 
  - **Richness**: total number of unique taxa
  - **Shannon index**: richness and evenness (how many taxa are present and how evenly their abundances are distributed; penalizes dominance)
  - **Simpson index**: the probability that two individuals randomly selected from a sample belong to the same species, thus emphasizing dominant taxa

These traditional diversity metrics assume an equal sampling effort, but micobiome data typically have uneven sequencing depths.

**Rarefaction**: subsampling of each sample to the same sequencing depth (typically that of the shallowest sample).
  - Rarefaction removes differences in library size (sequencing depth), with the goal of ensuring fair comparison between the samples (i.e., deeper samples won’t appear more diverse).
  - However, the use of rarefaction is disputed, because it discards data (statistical power is decreased by shallower samples), introduces random variation (the subsampling is stochastic), can bias diversity estimation (especially for rare taxa) and does not correct for unobserved species.

Alpha diversity values often exhibit heteroscedasticity (unequal variances across groups) and non-normal distributions. As a result, non-parametric statistical tests (**Wilcoxon Rank-Sum test** for comparing two groups and the **Kruskal-Wallis test** for comparing more than two groups) are typically used. These tests are based on ranked data rather than raw values, and do not assume normality. However, they do assume that the distributions being compared have a similar shape (e.g., similar variance and skewness).

To avoid rarefaction and to better model the underlying microbial diversity, newer methods infer latent (unseen) diversity using statistical methods.

**Chao1**: estimates total species richness. Chao1 infers the number of unseen species by using the observed counts of singletons and doubletons and the assumption that individuals are randomly and independently sampled from the community.

**Breakaway**: estimates true species richness by fitting a rational function model (a ratio of polynomials) to the frequency count data (frequency of the frequencies) using weighted nonlinear least squares to model the unobserved portion of the community (unseen taxa). Weights are assigned based on the estimated variance of each frequency count (i.e., high variance points like singletons receive less weight).

**DivNet**: estimates Shannon and Simpson diversity by modeling latent relative abundances (true but unobserved proportions) of the taxa in the community. It assumes that the additive log-ratio (alr)-transformed latent proportions follow a multivariate normal distribution and that the observed counts result from a multinomial sampling process of the inverse alr-transformed latent proportions. DivNet uses maximum likelihood estimation to jointly estimate the mean vector (μ) and the covariance matrix (Σ) that define the latent distribution, and from that, derives the diversity estimates (the latent proportions most likely to have produced the observed counts) and their associated uncertainty.

**Betta**: a regression framework that performs statistical testing on alpha diversity estimates produced by breakaway or DivNet, while explicitly accounting for sampling variability in those estimates. It uses the alpha diversity values and their standard errors (breakaway) or variance estimates (DivNet) to fit a linear model, where the data points are weighed with respect to the precision (inverse variance) of each estimate. This allows for hypothesis testing or covariate modeling that properly incorporate uncertainty due to unobserved taxa, sequencing depth, and variability in taxon detection.

## :dotted_line_face::corn: Beta-Diversity
Beta diversity measures between-sample differences in whole microbiome communities (it asks are samples from different groups more different than samples from within a given group).

**Bray-Curtis distance**: measures the compositional dissimilarity between two samples based on the abundances of taxa present in at least one of the samples. It is computed as the weighted sum of absolute differences, where weights are the abundances, thus the metric is dominated by abundant taxa.
  - Values range from 0 (samples are identical) to 1 (completely dissimilar/ no shared taxa)
  - **Pre-processing**: low prevalence taxa should be removed (sensitive to sparsity) and counts should be transformed to relative abundances to avoid bias from sequencing depth.
  - **Limitations**: does not explicitly account for compositionality and tends to be biased toward abundant taxa.

**Canberra distance**: measures compositional dissimilarity between two samples based on the abundances of taxa present in at least one of the samples. It sums the ratios of the absolute differences to the sums of the abundances for each taxon, thereby giving relatively equal weight to rare and abundant taxa (i.e., rare taxa contribute proportionally more than they would in Bray-Curtis).
  - Values range from 0 (samples are identical) to 1 (completely dissimilar/ no shared taxa)
  - **Pre-processing**: low prevalence taxa should be removed (sensitive to sparsity) and counts should be transformed to relative abundances to avoid bias from sequencing depth.
  **Limitations**: does not explicitly account for compositionality

**Jaccard distance**: measures the dissimilarity between two samples based on presence/absence of taxa, ignoring their abundances. It quantifies how many taxa are shared versus unique to each sample.
  - Values range from 0 (samples have identical sets of taxa) to 1 (no shared taxa)
  - **Pre-processing**: low prevalence taxa should be removed (sensitive to sparsity) and counts should be converted to presence/absence data (any taxon with a count > 0 becomes 1 (present), otherwise it becomes 0 (absent)). No normalization is needed since it does not depend on abundance. 
  - **Limitations**: ignores abundance information and presence/absence data can be heavily influenced by sampling noise and sequencing depth.

**Euclidean distance**: the geometric (straight-line) distance between two samples in multidimensional space, where each dimension corresponds to a feature and and the coordinate of a sample along each axis is determined by the abundance of that feature.
  - Values range from zero (samples are identical) to larger positive values (greater overall differences in abundance)
  - **Pre-processing**: low prevalence taxa should be removed (sensitive to sparsity) and counts should be log transformed (reduces the influence of very abundant taxa)
  - **Limitations**: sensitive to magnitude differences (differences are squared) and thus influenced by highly abundant taxa and sequencing depths.

**Aitchison distance**: the Euclidean distance between samples after centered log-ratio (CLR) transformation (which projects the compositional data from the simplex to Euclidean space).
  - Values range from zero (log-ratios are identical) to large values (greater differences in log-ratio structure)
  - **Pre-processing**: counts need to be CLR-transformed (handles the compositional constraint of microbiome data).
  - **Limitations**: interpretation is less intuitive (represents how much log-ratios between taxa, rather than absolute or relative abundances, differ between samples).

**Principal Coordinate Analysis (PCoA)**: an ordination method that represents samples in a low-dimensional, Euclidean space, while preserving the pairwise distances (or dissimilarities) between them as faithfully as possible. It assumes that the input distance matrix (not the raw data) can be faithfully represented in Euclidean space. The resulting axes represent the amount of variation in the distance matrix explained by each coordinate (interpretable when using metric distances).

**Non-metric Multidimensional Scaling (NMDS)**: an ordination method that represents samples in a low-dimensional space based on rank orders of pairwise distances (not the distances themselves). NMDS does not assume that the data can be projected into Euclidean space and calculates a stress value to quantify how well the configuration preserves the rank-order of the original dissimilarities. It works with any distance metric, but the axes have no direct meaning (no percentage of variance explained).

**Principal Component Analysis (PCA)**: an ordination method that represents samples in a low-dimensional, Euclidean space by preserving the variance in the original feature table (e.g., taxa abundances, not pairwise distances). PCA assumes the data lie in Euclidean space and that linear combinations of features meaningfully capture variation. PCA is valid only when data have been transformed appropriately (e.g., CLR-transformed data). The resulting axes are ordered by the amount of variance they explain in the data.

**DivNet**: estimates Bray-Curtis and Euclidean distances by modeling latent relative abundances (true but unobserved proportions) of the taxa in each community. It assumes that the additive log-ratio (alr)-transformed latent proportions follow a multivariate normal distribution and that the observed counts result from a multinomial sampling process of the inverse alr-transformed latent proportions. DivNet uses maximum likelihood estimation to jointly estimate the mean vector (μ) and the covariance matrix (Σ) that define the latent distribution. Bray-Curtis and Euclidean distances are then calculated from the inferred latent relative abundances, capturing both observed data and sampling variability of each sample. 

**testBetaDiversity**: simulates multiple plausible versions of the latent relative compositions for each sample from the estimated multivariate normal distribution. For each simulation, it calculates Bray-Curtis and Euclidean distances between samples, reflecting both observed and unobserved diversity. To test whether within-group versus between-group distances differ, it permutes the sample labels across the simulations to construct a null distribution. This approach explicitly accounts for sampling uncertainty and compositional structure. 

**PERMANOVA**: uses a single, fixed distance matrix computed from observed data (doesn’t account for sampling variability or latent uncertainty). In order to test whether within-group versus between-group distances differ, a null distribution is generated by permuting sample labels. PERMANOVA assumes equal dispersion (variance) across groups. If this assumption is violated (e.g., one group is more dispersed), it can yield false positives even when group centroids are not meaningfully different.

## :octopus::cloud_with_snow: Differential Abundance Analysis
The identification of individual microbial taxa whose relative abundances differ significantly between groups. 

**ALDEx2 (ANOVA-Like Differential Expression)**: assumes raw counts are the result of a random sampling process from an underlying set of true feature abundances that can be modeled using a multinomial distribution (i.e., the observed counts are drawn based on the true (but unknown) proportions of each feature). Since the true feature abundances are unknown, ALDEx2 uses a Dirichlet distribution to model the uncertainty about what the true proportions could be given the observed counts. The Dirichlet distribution represents a probability distribution over possible parameter values of the multinomial distributions (i.e., over plausible feature proportions). It then uses Monte Carlo sampling to draw 128 (by default) probability vectors from the Dirichlet posterior, each representing a plausible instance of the underlying feature composition. The 128 sampled compositions are transformed using the centered log-ratio (CLR) transformation to project the data into an unconstrained (Euclidean) space. The Wilcoxon test is then applied to each feature in each of the 128 Monte Carlo instances and the results are aggregated to produce estimates of the expected effect sizes (median per feature CLR differences between groups across all Monte Carlo instances) and p-values. Multiple testing correction is performed using the Benjamini-Hochberg FDR procedure.

**MaAsLin2 (Multivariable Association with Linear Models)**: normalizes and transforms the count data according to user defined values. It is not inherently compositionally aware, but if normalization is set to CLR, MaAsLin2 will use total sum scaling to convert the data to relative abundances (which adjusts for differences in sequencing depth) and then will apply CLR transformation (which projects the data from the simplex into Euclidean space, allowing valid application of linear models). MaAsLin2 fits a separate general linear model to each microbial feature to model how its abundance varies with respect to metadata variables (e.g., condition, sex, BMI), while adjusting for potential confounders. MaAsLin2 assumes that the relationship between metadata and abundance is linear, that samples are independent (unless modeled with random effects), that there is no multicollinearity (i.e., covariates are not highly correlated), and that residuals are normally distributed and homoscedastic (features have similar variances). To test whether a covariate significantly affects abundance, MaAsLin2 uses t-tests with Satterthwaite-approximated degrees of freedom if random effects are included, and standard t-tests if the models do not include random effects to assess the significance of the regression coefficients. The effect size is the regression coefficient from the model fit for each feature and covariate. Multiple testing correction is performed using the Benjamini-Hochberg FDR procedure.

**Corncob (COunt RegressioN for Correlated Observations with the Beta-binomial)**: assumes that the true underlying relative abundance of a taxon across samples follows a beta distribution (a probability distribution defined by two shape parameters) and that the observed count of a taxon in a given sample can be modeled as a binomial draw from the total number of reads in that sample, using the sample-specific true abundance as the success probability. For each taxon, the mean (abundance) and the dispersion (variability) parameters of the beta-distribution are modeled using logistic regression and estimated by iteratively maximizing a log-likelihood function (i.e., finding the parameter values that maximize the probability of observing the actual data), since the beta-binomial model does not have a closed-form solution. Corncob uses the BFGS optimization algorithm, which approximates the Hessian matrix, a matrix of second-derivatives of the log-likelihood with respect to the model parameters (representative of the curvature of the log-likelihood surface and reflective of the confidence of the model in the parameter estimates). To test whether a covariate significantly affects the abundance and/or variability, corncob compares the log likelihoods of the full model (with covariate) to the reduced model (without covariate) using the likelihood ratio test (LRT) and evaluates the resulting test statistic using parametric bootstrapping or comparison to a chi-squared distribution. The effect size is the coefficient from the beta regression on the logit scale (i.e., the log odds change in relative abundance per unit change in the covariate). Multiple testing correction is performed using the Benjamini-Hochberg FDR procedure. 

**DESeq2**: accounts for differences in sequencing depth by computing a size factor for each sample, absed on the assumption that most taxa are not differentially abundant. For each sample, the count for each taxon is divided by the geometric mean for that taxon across all the samples, and the median of these ratios in a given sample is the size factor for that sample. The counts in each sample are then divided by the size factor to get normalized counts and these counts are assumed to follow a negative binomial distribution (a Poisson distribution with an extra parameter for dispersion). Since dispersion and log fold change (effect size) can be noisy, especially for low abundance taxa and for small sample sizes, DESeq2 shrinks (regularizes) the dispersion estimate for each taxon toward the overall dispersion-mean trend line (for all taxa) using empirical Bayes shrinkage based on the precision of the dispersion estimate for each taxon and shrinks the log fold changes toward zero based on their uncertainty (standard error). To test whether a covariate significantly affects the abundance, DESeq2 compares the group variable coefficient of the full model using the Wald test (test statistic assumed to follow a standard normal distribution) or compares the log likelihoods of the full model (with covariate) to the reduced model (without covariate) using the LRT (test statistic assumed to follow a chi-squared distribution). Multiple testing correction is performed using the Benjamini-Hochberg FDR procedure.

<!-- ** ANCOM-BC (Analysis of Composition of Microbiomes with Bias Correction)**: assumes that there is a log-linear relationship between absolute abundances and covariates, so the observed counts are log-transformed after the addition of a pseudocount. The log observed counts are modeled as the sum of the log true absolute abundance of each axon, a sample-specific bias term (a scaling factor) and random error. To estimate the bias term, ANCOM-BC uses the assumption that most taxa are not differentially abundant between groups (i.e., systemic differences are likely due to sample-specific biases, like sequencing depth or compositional effects). This estimated bias is then subtracted from the observed counts in order to better approximate the true log absolute abundance of each taxon. ANCOM-BC then fits a linear model predicting the bias-corrected log absolute abundance of each taxon as a function of covariates (including the group variable), with a log(library size) offset included to explicitly adjust for differences in sequencing depth. ANCOM-BC uses robust variance estimation (heteroskedasticity-consistent standard errors) to obtain reliable standard errors, given that microbiome data often show unequal variances (heteroskedasticity). The Wald test is then applied to the regression coefficient for the group variable using the robust standard errors to test whether the group significantly affects the abundance of the taxon. The effect size is the estimated bias-corrected log fold change, which is interpreted as the log difference in true absolute abundance between groups. Multiple testing correction is performed using the Benjamini-Hochberg FDR procedure.-->

<!-- Taxa with low counts but large corrected log-fold changes may be significant in ANCOM-BC, even if they are not detected by methods like DESeq2, corncob, or ALDEx2. ANCOM-BC assumes a linear model on bias-corrected log counts, and uses robust SEs, so it can detect large log-fold changes even in low-abundance taxa, as long as the standard error is relatively small. Is more sensitive than conservative methods like ALDEx2 and does not downweight sparse or zero-heavy taxa the way negative binomial models might. ANCOM-BC does not model zero inflation or overdispersion explicitly, so it might treat zero-heavy, low-abundance taxa as stable if the log-abundances appear consistent post-bias-correction, even if this is just random. -->

## :unicorn::smiling_imp: sPLS-DA and DIABLO
**sPLS-DA (sparse Partial Least Squares Discriminant Analysis)** is a supervised, multivariate method to reduce dimensionality, classify samples based on a categorical outcome and select the most discriminative features. The method works well with data that is high-dimensional and/or multicollinear.
  - Dimensionality is reduced by projecting the data onto a few latent components
and through feature selection (only the most informative variables per
component are retained).
  - Multicollinearity does not cause instability because the method does not require inversion of the covariance matrix (sPLS-DA operates in latent space rather than directly estimating coefficients from the raw variable space). 
  - Weighted linear combinations of features are constructed and the combined signal of multiple correlated features is shared across the latent component, which avoids arbitrarily favoring one of the correlated features over the others.

**DIABLO (Data Integration for Biomarker discovery using Latent cOmponents)** extends the framework of sPLS-DA to integrate multiple omics datasets (blocks) measured on the same samples.
  - DIABLO constructs latent components in a manner similar to that of sPLS-DA, but the components in DIABLO also maximize the correlation between the data blocks in addition to separating the classes.

Data used in both sPLS-DA and DIABLO analysis should be scaled to unit variance to ensure that high-variance variables do not dominate. This is particularly important for multi-block analysis, since different data types can have very different levels of variance.

Both sPLS-DA and DIABLO estimate regression coefficients from latent features, not directly from the original features (DIABLO estimates the coefficients independently for each omics block). The latent components are computed as weighted linear combinations of the original features that maximize the covariance between the predictors and a dummy-coded class membership matrix (which optimizes the separation of the classes in the latent space) and in the case of DIABLO, also maximizes the correlation between component across blocks (as specified by a user-designed design matrix).

Soft-thresholding is applied to retain only the top ```keepX``` features per component that contribute the most strongly to class separation (i.e., those that have the highest absolute weights/loadings) and additionally to inter-block correlation in the case of DIABLO. The weights of the selected features are then iteratively optimized to maximize class separation (and agreement between omics layers for DIABLO) in the latent component space (shared latent space for DIABLO). 

Once convergence is reached for a given component (i.e., when the features weights stabilize below a set tolerance or a set number of iterations is reached), the explained variance is removed, and a new orthogonal component is estimated to capture the remaining variance that was not explained by the earlier components.
