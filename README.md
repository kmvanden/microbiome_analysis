# Microbiome Analysis
## :microbe::chart_with_upwards_trend: Why Microbiome Data Are Statistically Challenging
**Compositionality**: microbiome data represent relative abundances, not absolute counts, which means that the data are constrained. Since all values are non-negative and sum to a constraint, the data exist in a simplex, not in standard Euclidean space. Furthermore, since the total abundance is fixed, spurious correlations and dependencies are created between taxa (i.e., an increase in the proportion of one taxa must be accompanied by decreases in the others, even if their absolute abundances did not change). Many statistical models assume that variables are independent and unconstrained in Euclidean space, and microbiome data violate these assumptions.
  - **Solution**: Log-ratio transformations move compositional data from the simplex into a standard Euclidean (unconstrained) space suitable for standard statistics. Alternatively, some methods model compositionality explicitly, like ALDEx2, which uses CLR transformation and Monte Carlo sampling from a Dirichlet-multinomial distribution.

**High dimensionality**: microbiome datasets typically contain many more features (taxa) than samples (p >> n), whereas most standard statistical tests assume that the number of samples exceeds the number of features. In small datasets, chance fluctuations in relative abundances can appear as significant differences and high numbers of features relative to the number of samples increases the risk of this happening. Additionally, with high dimensional data, the regression problem becomes undetermined: you have more unknowns (coefficients) than equations (observations), so there are an infinite number of solutions for the coefficients. In logistic regression, this creates a flat optimization surface (ill-conditioned), which can result in non-convergence, numerical instability (very large coefficients) and overfitting.
  - **Solution**: Dimensionality reduction techniques (e.g., PCA or PCoA) reduce the number of features to a smaller set of composite (latent) features. Feature selection by filtering low-abundance or low-prevalence taxa, regularization techniques (e.g., Lasso) or recursive feature elimination can help to identify a smaller subset of features to be used in subsequent analyses.

**Overdispersion and zero-inflation**: in microbiome data, most taxa are absent (zero counts) in most samples, due to both rare taxa being below detection thresholds (rounded or sampling zeros) and due to true biological absence of certain taxa (true or structural zeros). Sparsity is a major contributor to the overdispersion observed in microbiome count data: a species may be absent in most samples, but have very large values in other samples when it is present. Overdispersion violates the assumptions made by models used with count data, like the Poisson distribution, which assume that the variance does not exceed the mean. Additionally, the sparsity of microbiome data also results in zero-inflation (i.e., more zeros than expected under typical count distributions), due to the presence of both sampling and structural zeros.
  - **Solution**: negative binomial models (e.g., in DESeq2) have a dispersion parameter that allows them to model variance independently from the mean and to therefore capture the overdispersion present in microbiome data. Zero-inflation can also be controlled for by using zero-inflated negative binomial (ZINB) models, which explicitly model the zero counts as separate processes (a structural zero process and a negative binomial count process). Additionally, feature filtering before analysis can help to reduce the impact of zero-inflation. 

## :moneybag::balance_scale: Alpha Diversity
Alpha diversity measures the diversity within a single sample. Traditional alpha diversity metrics come from classical ecology and include: 
  - **Richness**: total number of unique taxa
  - **Shannon index**: richness and evenness (how many taxa are present and how evenly their abundances are distributed; penalizes dominance)
  - **Simpson index**: the probability that two individuals randomly selected from a sample belong to the same species, thus emphasizing dominant taxa

These traditional diversity metrics assume an equal sampling effort, but micobiome data typically have uneven sequencing depths.

**Rarefaction**: subsampling of each sample to the same sequencing depth (typically that of the shallowest sample).
  - Rarefaction removes differences in library size (sequencing depth), with the goal of ensuring fair comparison between the samples (i.e., deeper samples won’t appear more diverse).
  - However, the use of rarefaction is disputed, because it discards data (statistical power is decreased by shallower samples), introduces random variation (the subsampling is stochastic), can bias diversity estimation (especially for rare taxa) and does not correct for unobserved species.

To avoid rarefaction and to better model the underlying microbial diversity, newer methods infer latent (unseen) diversity using statistical methods.

**Chao1**: estimates total species richness. Chao1 infers the number of unseen species by using the observed counts of singletons and doubletons and the assumption that individuals are randomly and independently sampled from the community.

**Breakaway**: estimates true richness by fitting a rational function model (a ratio of polynomials) to the frequency count data (frequency of the frequencies) using weighted nonlinear least squares to estimate the unobserved portion of the community (unseen taxa). This regression assigns weights to each of the frequencies based on its estimated variance (i.e., high variance points like singletons get less weight).

**DivNet**: estimates Shannon and Simpson diversity from inferred latent relative abundances (true but unobserved proportions). It assumes that the additive log-ratio (alr)-transformed latent proportions follow a multivariate normal distribution and models the observed counts as having arisen from a multinomial sampling process of the inverse alr-transformed latent proportions. DivNet uses maximum likelihood estimation to jointly estimate the mean vector (μ) and the covariance matrix (Σ) that define the multivariate normal distribution and the latent proportions most likely to have produced the observed counts. 
